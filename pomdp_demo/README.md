# Demonstra√ß√£o de POMDP - Grid World com Neblina

Projeto educacional para demonstrar o funcionamento de um POMDP (Partially Observable Markov Decision Process) na pr√°tica.

## O que √© POMDP?

Um POMDP √© um modelo matem√°tico para tomada de decis√£o sequencial em ambientes onde:
- O agente **n√£o observa diretamente** o estado do ambiente
- As observa√ß√µes s√£o **ruidosas ou parciais**
- O agente deve manter uma **cren√ßa (belief)** sobre o estado real
- A pol√≠tica √≥tima depende da distribui√ß√£o de probabilidade sobre estados

## O Problema

Neste exemplo, um agente deve navegar em um grid 5x5 para alcan√ßar um objetivo, mas:
- üå´Ô∏è H√° "neblina" - as observa√ß√µes t√™m 70% de chance de estar incorretas por padr√£o na demonstra√ß√£o (ajust√°vel)
- üß± Existem obst√°culos no caminho
- üéØ O agente deve usar o hist√≥rico de observa√ß√µes para inferir sua posi√ß√£o real

## Componentes

### 1. Ambiente (`environment.py`)
- Grid World 5x5
- Estados: posi√ß√µes (x, y)
- A√ß√µes: cima, baixo, esquerda, direita
- Observa√ß√µes: posi√ß√£o observada (pode ser ruidosa)
- Ru√≠do de transi√ß√£o: chance de escorregar para estado vizinho (slip)
- Modelo: T(s,a,s'), O(s',a,o), R(s,a)

### 2. Solver (`pomdp_solver.py`)
- Algoritmo: Value Iteration para POMDP
- Computa pol√≠tica √≥tima sobre belief states
- Usa alpha vectors para representar value function
- Atualiza belief usando filtro Bayesiano

### 3. Visualizador (`visualizer.py`)
- Visualiza√ß√£o em tempo real com Pygame
- Mostra:
  - Grid com obst√°culos e objetivo
  - Posi√ß√£o real do agente (c√≠rculo azul)
  - Observa√ß√£o recebida (c√≠rculo amarelo se incorreta, ciano se correta)
  - Belief state (heatmap vermelho no grid + barras laterais)
  - Informa√ß√µes (a√ß√£o, reward, etc.)

## Instala√ß√£o

```bash
# Instalar depend√™ncias
pip install -r requirements.txt
```

## Uso

```bash
# Executar demonstra√ß√£o
python main.py
```

O script ir√°:
1. Criar o ambiente Grid World
2. Treinar o solver POMDP
3. Executar um epis√≥dio completo
4. Visualizar em tempo real
5. Salvar v√≠deo em `pomdp_demonstration.mp4`

## Interpreta√ß√£o da Visualiza√ß√£o

### Grid
- ‚¨ú **Branco**: C√©lulas vazias (naveg√°veis)
- ‚¨õ **Cinza**: Obst√°culos
- üü© **Verde**: Objetivo (META)

### Agente e Observa√ß√µes
- üîµ **C√≠rculo azul**: Posi√ß√£o real do agente
- üü° **C√≠rculo amarelo (outline)**: Observa√ß√£o incorreta
- üî∑ **C√≠rculo ciano (outline)**: Observa√ß√£o correta

### Belief State
- üü• **Heatmap vermelho**: Probabilidade de estar em cada c√©lula
  - Mais intenso = maior probabilidade
- **Painel lateral**: Top 10 estados mais prov√°veis com barras

### Painel de Informa√ß√µes
- **Step**: N√∫mero do passo atual
- **A√ß√£o**: √öltima a√ß√£o executada
- **Observa√ß√£o**: Posi√ß√£o observada
- **Reward**: Recompensa do √∫ltimo passo
- **Total Reward**: Recompensa acumulada
- **Status**: Em execu√ß√£o ou conclu√≠do

## Conceitos Demonstrados

### 1. Belief State
O agente mant√©m uma distribui√ß√£o de probabilidade sobre sua posi√ß√£o real:
```
belief(s) = P(estado = s | hist√≥rico de observa√ß√µes)
```

### 2. Atualiza√ß√£o de Belief (Filtro Bayesiano)
Ap√≥s executar a√ß√£o `a` e observar `o`:
```
belief'(s') ‚àù O(s',a,o) √ó Œ£‚Çõ T(s,a,s') √ó belief(s)
```

### 3. Sele√ß√£o de A√ß√£o
A pol√≠tica √≥tima seleciona a√ß√µes baseadas no belief, n√£o no estado:
```
œÄ*(b) = argmax_a Q*(b, a)
```

### 4. Value Iteration para POMDP
- Computa value function sobre belief space
- Usa alpha vectors para representa√ß√£o compacta
- Converge para pol√≠tica √≥tima

## Par√¢metros Ajust√°veis

Em `main.py`, voc√™ pode modificar:
- `grid_size`: Tamanho do grid (default atual: 7 para a demonstra√ß√£o; ambiente aceita qualquer valor)
- `observation_noise`: Probabilidade de observa√ß√£o incorreta (default do script: 0.7; default do ambiente: 0.2 se instanciado diretamente)
- `transition_noise`: Probabilidade de escorregar para outro estado (default do script: 0.2; default do ambiente: 0.2)
- `n_iterations`: Itera√ß√µes de treinamento (default: 30)
- `gamma`: Fator de desconto (default: 0.95)
- `max_steps`: M√°ximo de passos por epis√≥dio (default: 50)

Em `environment.py`:
- `obstacles`: Lista de posi√ß√µes com obst√°culos
- `start_pos`: Posi√ß√£o inicial
- `goal_pos`: Posi√ß√£o objetivo

## Arquitetura do C√≥digo

```
pomdp_demo/
‚îú‚îÄ‚îÄ environment.py      # Ambiente Grid World POMDP
‚îú‚îÄ‚îÄ pomdp_solver.py     # Solver com Value Iteration
‚îú‚îÄ‚îÄ visualizer.py       # Visualiza√ß√£o com Pygame
‚îú‚îÄ‚îÄ main.py             # Script principal
‚îú‚îÄ‚îÄ requirements.txt    # Depend√™ncias
‚îî‚îÄ‚îÄ README.md           # Este arquivo
```